services:
  imagine:
    image: ${IMAGINE_IMAGE-himmelroman/tmunan_imagine:latest}
    #    restart: unless-stopped
    volumes:
      - ~/.cache/huggingface:/root/app/models
      - ~/.cache/tmunan/tensorrt:/root/app/tensorrt
      - ~/.aws:/root/.aws
    environment:
      - HF_HOME=/root/app/models
      - TENSORRT_DIR=/root/app/tensorrt
      - TMUNAN_IMAGE_MODE=${TMUNAN_IMAGE_MODE:-stream}
    ports:
      - "8090:8090"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    logging:
      driver: "json-file"
      options:
        max-file: "1"
        max-size: "100k"

  stream:
    image: ${STREAM_IMAGE-himmelroman/tmunan_stream:latest}
    #    restart: unless-stopped
    network_mode: "host"
    env_file: ".env"
    environment:
      - IMAGINE_HOST=localhost
      - IMAGINE_PORT=8090
      - ABLY_API_KEY=${ABLY_API_KEY}
      - SIGNALING_CHANNEL=${SIGNALING_CHANNEL-tmunan_dev}
#    ports:
#      - "8080:8080"
#      - "10000-20000:10000-20000/udp"
    depends_on:
      - imagine
    logging:
      driver: "json-file"
      options:
        max-file: "1"
        max-size: "100k"

  nginx:
    image: nginx
    #    restart: unless-stopped
    network_mode: "host"
#    ports:
#      - "80:80"
#      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - /etc/letsencrypt:/etc/letsencrypt
    depends_on:
      - imagine
      - stream
    logging:
      driver: "json-file"
      options:
        max-size: "100k"
        max-file: "1"
